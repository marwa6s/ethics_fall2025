---
title: 'BIAS Blog 1: Right to Fair Representation'
date: 2025-10-24
permalink: /posts/2025/10/BIAS-blog1/
tags:
  - Text-to-Image Modules
  - AI harms
  - BIAS Representations
  - Diversity
---

In a world where AI is increasingly influential and often used as a learning tool, it is crucial that these systems accurately represent marginalized groups. This article will specifically explore how AI misrepresents people of color and examine the broader harms that result from these misrepresentations.

**New Article**  
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)

The case study “AI’s Regimes of Representation” investigates how text-to-image models depict South Asian communities, highlighting how generative AI can misrepresent or erase marginalized cultures due to biased datasets and dominant cultural norms. Its goal is to explore these failures through a community-centered lens, emphasizing the lived experiences and perspectives of people whose cultures are represented.

I’ve spent much of my life noticing what’s not there. Growing up as a Black Muslim woman in the West, I rarely saw myself represented in media in ways that felt authentic. When I did, it was often through clichés or stereotypes; my identity flattened into a label, my experiences simplified, my humanity overlooked. So when I read the case study “AI’s Regimes of Representation”, which examines how text-to-image AI models depict South Asian communities, I felt a mix of recognition and unease. It reminded me that technology, like media, only reflects the biases and blind spots of its creators.

Cultural representation, to me, is more than images or words. It’s about truthfully reflecting people’s lives, their diversity, their agency. It’s the difference between seeing someone like me exist in the world or being reduced to a token, an object, or a stereotype.

If I were evaluating AI-generated images of myself, I’d center my race, my faith, my style, and the social spaces I inhabit. I’d ask: Does this AI understand that my identity is layered? That being Black and Muslim in the West comes with specific cultural practices, challenges, and joys? Does it honor my existence, or does it exoticize me, flatten me, or erase my complexity?

Representation is never static; it changes over time, differs across cultures, and varies even from person to person. But AI often tries to “freeze” identity into a dataset, a category, or a prompt. The challenge is how to encode this fluidity into models.
I think it’s possible if AI is built with feedback loops, participatory design, and adaptive datasets that allow for multiple, evolving interpretations. Encoding doesn’t have to mean fixing identity; it can mean creating systems that recognize and respect diversity and context.

Technology and media have long shaped how the world sees marginalized communities. From colonial imagery to modern digital stereotypes, the dominant gaze has often dictated representation. AI is the next frontier of this dynamic. If we want responsible, fair, and representative AI, we need to learn from history: we must center marginalized voices not just as participants, but as decision-makers in defining what counts as fair and accurate. We must question the assumptions baked into these systems and actively challenge the norms they reproduce.

How can AI developers ensure that those most marginalized in society have real authority in shaping how AI represents them, rather than being included only superficially?

I ask this because too often, inclusion is performative. Communities are consulted, but their perspectives don’t have real weight. Without decision-making power, the same biases persist, just under a veneer of diversity.

Writing this has made me confront something I already knew but often ignore: AI is not separate from society. It reflects who we are, what we value, and whose stories we choose to tell, or not tell. Thinking about representation through AI made me realize the stakes are high. Misrepresentation or erasure isn’t just frustrating, it shapes how people see us, understand us, and treat us in the real world.

This exercise has reminded me that technology can either perpetuate harm or offer visibility and dignity. And as someone whose life has too often been rendered invisible, I want to push for the latter. I want AI that sees me, not just as a token, but as a full, living, evolving person.