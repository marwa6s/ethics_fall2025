---
title: 'ALG Blog 1: Exception to Data Driven Rules'
date: 2025-09-25
permalink: /posts/2025/09/ALG-blog-1/
tags:
  - Data-Driven Exceptions
  - Algorithmic Bias
  - Data-Driven Decisions
---
In today’s world, decisions about our lives such as loans, jobs, or even criminal sentencing, are increasingly made by algorithms. But what happens when these data-driven rules don’t fit everyone? This blog explores a case study that highlights how people who don’t match the “average” can be unfairly affected.

**New Article**  
[The Right to Be an Exception to a Data-Driven Rule](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)

The case study is there to show, in a concrete and easy-to-picture way, why a data-driven rule can fail for certain people and why those people might need special consideration.
It takes an everyday scenario and demonstrates that even when a rule works well on average, it can still unfairly harm someone who is an exception. The authors use the case study to make their main point feel real: that people should have the right to be treated differently when the general rule doesn’t fit their particular situation.

A data-driven rule is a decision based on patterns in past data. Being a data-driven exception means the rule doesn’t fit me because my situation is different from what the data expects.
An exception is not the same as an error—sometimes the model works “on average” but still gets my case wrong.
Humans can understand context, culture, and personal stories. Algorithms can’t do that, they rely only on numbers, which often reflect old biases, especially against people like Black, Muslim, or immigrant communities. And when an algorithm is wrong, it can hurt many people at once.
Some benefits of Individualization include being judged by my real situation, not by stereotypes. It can lead to fairer decisions. While the downsides is that it may require sharing more personal data, human reviewers might still bring bias, and it takes more time and effort.
Uncertainty shows when a model might not actually understand my case. That’s important because people like me are often underrepresented or misjudged in data. In high-stakes situations like sentencing, no single metric is good enough. Even one wrong decision can change a person’s life, so we need more than just accuracy, we need caution and human judgment.

Think of a time when you felt misjudged because someone assumed something about you based on your background, appearance, or identity. How does that experience help you understand the idea of being a “data-driven exception”?
I chose this question because the article shows how data-driven rules can misrepresent people whose lives don’t match the patterns in the data. Asking readers to connect this to a personal moment of feeling misjudged makes the idea more relatable. Many people have experienced unfair assumptions, so this question helps them emotionally connect with what it means when an algorithm does the same thing at a larger scale.

This exercise helped me think more deeply about how data-driven rules actually affect real people. Writing my own discussion question made me focus on what part of the reading felt most important and how to help others connect with it too. It made me realize that understanding these concepts isn’t just about the technical ideas—it's also about recognizing the human experiences behind them and encouraging others to reflect on their own.